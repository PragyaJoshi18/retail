import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('energy_data.csv')

# Split the data into features and target
X = data.drop('Theft', axis=1)
y = data['Theft']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create new features
X_train['AvgUsagePerMonth'] = X_train['TotalUsage'] / X_train['Months']
X_test['AvgUsagePerMonth'] = X_test['TotalUsage'] / X_test['Months']


from xgboost import XGBClassifier

# Initialize the XGBClassifier model
model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Fit the model on the training data
model.fit(X_train, y_train)




from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score

# Make predictions on the test data
y_pred = model.predict(X_test)

# Calculate the evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

# Print the evaluation metrics
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'ROC AUC: {roc_auc:.2f}')


# Print the feature importances
importances = model.feature_importances_
for feature, importance in zip(X.columns, importances):
    print(f'{feature}: {importance:.2f}')

